Santiago LangChain tutorial:
https://www.youtube.com/watch?v=RWo4GDTZIsE

new env 
- python 3.10
- langchain 1.0 (pre release at tutorial date)

1) Setup Chatbot model=OpenAI
- Prompt input/output
- GPT-turbo model
- instruction to answer question based on provided context

2) Website > Vector database
- Take website context
- Divide into chunks
- Create OpenAI embeddings for the chunks
- can find answers based on similarity of embeddings from the vector store Database

Chunk size: 1000 -> 1 document that will be stoed
e.g. first 1000 characters will be 1 document that will be stored in my Vector DB, as a embedding
Chunk overlap: 200

Process flow
1) User ask question > generate embedding for that question
2) Search the Vector DB with the Question-embedding > Return similar documents to the Question
3) Returned Documents become context for GPT3.5 to answer the question

Export environment
conda env export --name langflow_env --file langflow_env_v2.yml
pip freeze > langflow_env_v2.txt
